{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e28369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishtiyak\\anaconda3\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ishtiyak\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishtiyak\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your query: give me coursera overview\n",
      "Response in English: Coursera is a prominent online learning platform founded in 2012 by Stanford professors Andrew Ng and Daphne Koller. It collaborates with universities and organizations globally to provide courses, certifications, and degrees in various fields. Coursera aims to offer accessible, high-quality education to learners worldwide for personal and professional development.\n",
      "\n",
      "Here are some key statistics:\n",
      "- Total Users: 100+ million learners\n",
      "- Course Offerings: 7,000+ courses\n",
      "- Partner Institutions: 275+ leading universities and companies\n",
      "- Specializations: 600+ programs for in-depth learning\n",
      "- Professional Certificates: 100+ offerings tailored to job-ready skills\n",
      "- Degrees Offered: Fully accredited Bachelor's and Master's degrees\n",
      "\n",
      "Coursera offers individual courses, specializations, professional certificates, MasterTrackâ„¢ certificates contributing to Master's degrees, online degrees, and guided projects. The platform features various courses across fields like Data Science & Analytics, Computer Science & Programming, Business & Management,\n",
      "Do you want to translate the response? (yes/no): no\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import faiss\n",
    "\n",
    "# Your OpenAI API Key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "def extract_text_from_all_pdfs(folder_path):\n",
    "    documents = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                full_text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    full_text += page.extract_text()\n",
    "                documents[filename] = full_text\n",
    "    return documents\n",
    "\n",
    "# Function to chunk text\n",
    "def chunk_text_for_all_docs(documents, max_tokens=500):\n",
    "    all_chunks = {}\n",
    "    for filename, text in documents.items():\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        chunk = []\n",
    "        tokens_count = 0\n",
    "        for sentence in sentences:\n",
    "            tokens = len(sentence.split())\n",
    "            if tokens_count + tokens > max_tokens:\n",
    "                chunks.append(\" \".join(chunk))\n",
    "                chunk = []\n",
    "                tokens_count = 0\n",
    "            chunk.append(sentence)\n",
    "            tokens_count += tokens\n",
    "        if chunk:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "        all_chunks[filename] = chunks\n",
    "    return all_chunks\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings_for_all_docs(all_chunks):\n",
    "    all_embeddings = {}\n",
    "    embedding_ids = []\n",
    "    chunk_count = 0\n",
    "    for filename, chunks in all_chunks.items():\n",
    "        embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "        all_embeddings[filename] = embeddings\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            embedding_ids.append(f\"{filename}-chunk-{i}\")\n",
    "            chunk_count += 1\n",
    "    return all_embeddings, embedding_ids\n",
    "\n",
    "# Function to create FAISS index\n",
    "def create_faiss_index(embeddings):\n",
    "    dimension = embeddings[next(iter(embeddings))][0].shape[0]  # Embedding size\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "    all_embedding_list = []\n",
    "    for embedding_list in embeddings.values():\n",
    "        all_embedding_list.extend(embedding_list)\n",
    "    index.add(np.array(all_embedding_list))\n",
    "    return index\n",
    "\n",
    "# Function to perform FAISS query\n",
    "def query_faiss(query, all_chunks, index, embedding_ids, top_k=3):\n",
    "    query_embedding = model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding), top_k)\n",
    "    retrieved_chunks = [all_chunks[embedding_ids[i].split('-chunk-')[0]][int(embedding_ids[i].split('-chunk-')[-1])] for i in I[0]]\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Function to generate response with GPT-3.5\n",
    "def generate_response_with_context(query, retrieved_chunks):\n",
    "    prompt = f\"User query: {query}\\n\\nRelevant information from documents:\\n{retrieved_chunks}\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Function to translate the text\n",
    "def translate_text(text, target_language):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"Translate this text to {target_language}.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Main chatbot function\n",
    "def rag_chatbot(all_chunks, index, embedding_ids):\n",
    "    user_query = input(\"Please enter your query: \")\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    retrieved_chunks = query_faiss(user_query, all_chunks, index, embedding_ids)\n",
    "    \n",
    "    # Generate response\n",
    "    response = generate_response_with_context(user_query, retrieved_chunks)\n",
    "    print(f\"Response in English: {response}\")\n",
    "    \n",
    "    # Ask user for language preference\n",
    "    translate_option = input(\"Do you want to translate the response? (yes/no): \")\n",
    "    if translate_option.lower() == 'yes':\n",
    "        target_language = input(\"Enter target language (e.g., 'French', 'Spanish', 'German'): \").lower()\n",
    "        translated_response = translate_text(response, target_language)\n",
    "        print(f\"Translated Response in {target_language}: {translated_response}\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = 'C:/Users/Ishtiyak/Desktop/chatbot/documents'  # Update this with your actual folder path\n",
    "documents = extract_text_from_all_pdfs(folder_path)\n",
    "all_chunks = chunk_text_for_all_docs(documents)\n",
    "all_embeddings, embedding_ids = generate_embeddings_for_all_docs(all_chunks)\n",
    "index = create_faiss_index(all_embeddings)\n",
    "\n",
    "# Run the chatbot\n",
    "rag_chatbot(all_chunks, index, embedding_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc52a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=f\"Summarize this: {text}\",\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2143e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
